{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import tensorflow as tf  \n",
    "import urllib.request as request  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download dataset\n",
    "IRIS_TRAIN_URL = \"churn-einzelhandel-train.csv\"  \n",
    "IRIS_TEST_URL = \"churn-einzelhandel-test.csv\"\n",
    "\n",
    "names = ['churn_flag', 'timezone_cd', 'tenure_visit', 'branch', 'abc_cd', 'abc_detailed_cd', 'avg_no_main_cat_1y', 'avg_no_main_cat_q1', 'avg_no_main_cat_q2', 'avg_no_main_cat_th', 'avg_no_main_cat_lh', 'avg_articles_6_6', 'avg_articles_q1_q2', 'ret_visits_per_chg_q1_q2', 'ret_sales_per_chg_q1_q2', 'visit_gap_ratio_th', 'visit_gap_ratio_1y', 'visit_gap_ratio_per_chg_6_6', 'avg_days_between_visits', 'home_st_visits_per_q1', 'home_st_visits_per_6_6', 'basket_spend', 'basket_spend_per_chg_6_6', 'basket_spend_per_chg_q1_q2', 'f_sales_per_chg_6_6', 'nf_visits_per_chg_6_6', 'nf_sales_per_chg_6_6', 'p_visits_per_chg_6_6', 'p_sales_per_chg_6_6', 'ret_visits_per_chg_6_6', 'ret_sales_per_chg_6_6', 'f_visits_per_chg_q1_q2', 'nf_visits_per_chg_q1_q2', 'nf_sales_per_chg_q1_q2', 'p_visits_per_chg_q1_q2', 'p_sales_per_chg_q1_q2', 'consistency_mnth_cd', 'consistency_qtr_cd', 'distinct_weeks_bought', 'expect_visit_flag', 'expect_visit_flag2', 'promo_sales_per', 'margin_per', 'food_sales_per', 'food_promo_per', 'nf_promo_per', 'food_colli_per', 'food_pieces_per', 'sales_last_model_period_per', 'visits_last_model_period_per', 'distinct_stores', 'recency', 'margin_1y', 'home_visits_q1', 'home_visits_lh', 'home_visits_th', 'visit_gap_th', 'visit_gap_lh', 'visit_gap_1y', 'promo_sales_1y', 'promo_sales_q1', 'promo_sales_q2', 'promo_sales_th', 'promo_sales_lh', 'nf_promo_sales', 'food_promo_sales', 'promo_visits_q1', 'promo_visits_q2', 'promo_visits_th', 'promo_visits_lh', 'promo_visits_1y', 'nf_sales_1y', 'nf_sales_q1', 'nf_sales_q2', 'nf_sales_th', 'nf_sales_lh', 'nf_visits_q1', 'nf_visits_q2', 'nf_visits_th', 'nf_visits_lh', 'food_sales_1y', 'food_sales_th', 'food_sales_lh', 'food_visits_1y', 'food_visits_q1', 'food_visits_q2', 'pieces', 'food_colli', 'food_pieces', 'colli_1y', 'colli_q1', 'home_store_visits_q1', 'home_store_visits_th', 'home_store_visits_lh', 'visits_q1', 'visits_q2', 'visits_q3', 'visits_q4', 'visits_1y', 'visits_th', 'visits_lh', 'sales_1y', 'sales_q1', 'sales_q2', 'sales_q3', 'sales_q4', 'sales_th', 'sales_lh', 'ret_visits_q1', 'ret_visits_q2', 'ret_visits_th', 'ret_visits_lh', 'ret_sales_q1', 'ret_sales_q2', 'ret_sales_th', 'ret_sales_lh', 'ret_sales_1y', 'prediction_week']  \n",
    "train = pd.read_csv(IRIS_TRAIN_URL, names=names, skiprows=0)  \n",
    "test = pd.read_csv(IRIS_TEST_URL, names=names, skiprows=0)\n",
    "\n",
    "# Train and test input data\n",
    "Xtrain = train.drop(\"churn_flag\", axis=1)  \n",
    "Xtest = test.drop(\"churn_flag\", axis=1)\n",
    "\n",
    "# Encode target values into binary ('one-hot' style) representation\n",
    "ytrain = pd.get_dummies(train.churn_flag)  \n",
    "ytest = pd.get_dummies(test.churn_flag)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create and train a tensorflow model of a neural network\n",
    "def create_train_model(hidden_nodes, num_iters):\n",
    "\n",
    "    # Reset the graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Placeholders for input and output data\n",
    "    X = tf.placeholder(shape=(14854, 117), dtype=tf.float64, name='X')\n",
    "    y = tf.placeholder(shape=(14854, 2), dtype=tf.float64, name='y')\n",
    "    \n",
    "    # Variables for two group of weights between the three layers of the network\n",
    "    W1 = tf.Variable(np.random.rand(117, hidden_nodes), dtype=tf.float64)\n",
    "    print('W1', W1)\n",
    "    W2 = tf.Variable(np.random.rand(hidden_nodes, 2), dtype=tf.float64)\n",
    "    print('W2', W2)\n",
    "\n",
    "    # Create the neural net graph\n",
    "    A1 = tf.sigmoid(tf.matmul(X, W1))\n",
    "    print('A1', A1)\n",
    "    print('y', y)\n",
    "    y_est = tf.sigmoid(tf.matmul(A1, W2))\n",
    "    \n",
    "    # Define a loss function\n",
    "    deltas = tf.square(y_est - y)\n",
    "    loss = tf.reduce_sum(deltas)\n",
    "\n",
    "    # Define a train operation to minimize the loss\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.005)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # Initialize variables and run session\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Go through num_iters iterations\n",
    "    for i in range(num_iters):\n",
    "        sess.run(train, feed_dict={X: Xtrain, y: ytrain})\n",
    "        loss_plot[hidden_nodes].append(sess.run(loss, feed_dict={X: Xtrain.as_matrix(), y: ytrain.as_matrix()}))\n",
    "        weights1 = sess.run(W1)\n",
    "        weights2 = sess.run(W2)\n",
    "        \n",
    "    print(\"loss (hidden nodes: %d, iterations: %d): %.2f\" % (hidden_nodes, num_iters, loss_plot[hidden_nodes][-1]))\n",
    "    sess.close()\n",
    "    return weights1, weights2\n",
    "  \n",
    "  \n",
    "# Run the training\n",
    "\n",
    "# Plot the loss function over iterations\n",
    "num_hidden_node1 = [50, 100, 200]\n",
    "loss_plot = {50: [], 100: [], 200: []}  \n",
    "weights1 = {10: None, 100: None, 200: None}  \n",
    "weights2 = {20: None, 100: None, 200: None} \n",
    "num_iters = 10\n",
    "\n",
    "plt.figure(figsize=(12,8))  \n",
    "for hidden_nodes in num_hidden_nodes:  \n",
    "    weights1[hidden_nodes], weights2[hidden_nodes] = create_train_model(hidden_nodes, num_iters)\n",
    "    plt.plot(range(num_iters), loss_plot[hidden_nodes], label=\"nn: 80-%d-30\" % hidden_nodes)\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=12)  \n",
    "plt.ylabel('Loss', fontsize=12)  \n",
    "plt.legend(fontsize=12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#more.....\n",
    "\n",
    "# Create and train a tensorflow model of a neural network\n",
    "def create_train_model(hidden_nodes, num_iters):\n",
    "\n",
    "    # Reset the graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Placeholders for input and output data\n",
    "    X = tf.placeholder(shape=(14854, 117), dtype=tf.float64, name='X')\n",
    "    y = tf.placeholder(shape=(14854, 2), dtype=tf.float64, name='y')\n",
    "    \n",
    "    # Variables for two group of weights between the three layers of the network\n",
    "    Wtest = tf.Variable(np.random.rand(117, hidden_nodes[2]), dtype=tf.float64)\n",
    "    W1 = tf.Variable(np.random.rand(117, hidden_nodes[0]), dtype=tf.float64)\n",
    "    print('W1', W1)\n",
    "    W2 = tf.Variable(np.random.rand(hidden_nodes[0], hidden_nodes[1]), dtype=tf.float64)\n",
    "    W3 = tf.Variable(np.random.rand(hidden_nodes[1], hidden_nodes[2]), dtype=tf.float64)\n",
    "    W4 = tf.Variable(np.random.rand(hidden_nodes[2], 2), dtype=tf.float64)\n",
    "    print('W4', W4)\n",
    "\n",
    "    # Create the neural net graph\n",
    "    A1 = tf.sigmoid(tf.matmul(X, Wtest))\n",
    "    print('A1', A1)\n",
    "    A2 = tf.sigmoid(tf.matmul(W1, W2))\n",
    "    print('A2', A2)\n",
    "    A3 = tf.sigmoid(tf.matmul(W2, W3))\n",
    "    print('A3', A3)\n",
    "    print('y', y)\n",
    "    y_est = tf.sigmoid(tf.matmul(A1, W4))\n",
    "    print('y_est', y_est)\n",
    "    \n",
    "    \n",
    "    # Define a loss function\n",
    "    deltas = tf.square(y_est - y)\n",
    "    print('deltas', deltas)\n",
    "    loss = tf.reduce_sum(deltas)\n",
    "\n",
    "    # Define a train operation to minimize the loss\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.005)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # Initialize variables and run session\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Go through num_iters iterations\n",
    "    for i in range(num_iters):\n",
    "        sess.run(train, feed_dict={X: Xtrain, y: ytrain})\n",
    "        loss_plot[hidden_nodes].append(sess.run(loss, feed_dict={X: Xtrain.as_matrix(), y: ytrain.as_matrix()}))\n",
    "        weights1 = sess.run(W1)\n",
    "        weights2 = sess.run(W2)\n",
    "        weights3 = sess.run(W3)\n",
    "        weights4 = sess.run(W4)\n",
    "        \n",
    "    print(\"loss (hidden nodes: %d, iterations: %d): %.2f\" % (hidden_nodes, num_iters, loss_plot[hidden_nodes][-1]))\n",
    "    sess.close()\n",
    "    return weights1, weights2, weights3, weights4\n",
    "  \n",
    "  \n",
    "# Run the training\n",
    "\n",
    "# Plot the loss function over iterations\n",
    "hidden_nodes = [70, 100, 40]\n",
    "loss_plot = {50}  \n",
    "weights1 = {10}  \n",
    "weights2 = {10}  \n",
    "weights3 = {10}  \n",
    "weights4 = {10}  \n",
    "num_iters = 10\n",
    "\n",
    "plt.figure(figsize=(12,8))  \n",
    "weights1[hidden_nodes[0]], weights2[hidden_nodes[1]], weights3[hidden_nodes[2]], weights4[hidden_nodes[2]] = create_train_model(hidden_nodes, num_iters)\n",
    "plt.plot(range(num_iters), loss_plot[hidden_nodes], label=\"nn: 117-%d-2\" % hidden_nodes)\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=12)  \n",
    "plt.ylabel('Loss', fontsize=12)  \n",
    "plt.legend(fontsize=12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate models on the test set\n",
    "X = tf.placeholder(shape=(4951, 117), dtype=tf.float64, name='X')  \n",
    "y = tf.placeholder(shape=(4951, 2), dtype=tf.float64, name='y')\n",
    "\n",
    "for hidden_nodes in num_hidden_nodes:\n",
    "\n",
    "    # Forward propagation\n",
    "    W1 = tf.Variable(weights1[hidden_nodes])\n",
    "    W2 = tf.Variable(weights2[hidden_nodes])\n",
    "    A1 = tf.sigmoid(tf.matmul(X, W1))\n",
    "    y_est = tf.sigmoid(tf.matmul(A1, W2))\n",
    "\n",
    "    # Calculate the predicted outputs\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        y_est_np = sess.run(y_est, feed_dict={X: Xtest, y: ytest})\n",
    "\n",
    "    #Calculate the prediction accuracy\n",
    "    correct = [estimate.argmax(axis=0) == target.argmax(axis=0) \n",
    "               for estimate, target in zip(y_est_np, ytest.as_matrix())]\n",
    "    accuracy = 100 * sum(correct) / len(correct)\n",
    "    print('Network architecture 80-%d-30, accuracy: %.2f%%' % (hidden_nodes, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "# Evaluate models on the test set\n",
    "X = tf.placeholder(shape=(4951, 117), dtype=tf.float64, name='X')  \n",
    "y = tf.placeholder(shape=(4951, 2), dtype=tf.float64, name='y')\n",
    "\n",
    "for hidden_nodes in num_hidden_nodes:\n",
    "\n",
    "    # Forward propagation\n",
    "    W1 = tf.Variable(weights1[hidden_nodes])\n",
    "    W2 = tf.Variable(weights2[hidden_nodes])\n",
    "    A1 = tf.sigmoid(tf.matmul(X, W1))\n",
    "    y_est = tf.sigmoid(tf.matmul(A1, W2))\n",
    "\n",
    "    # Calculate the predicted outputs\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        y_est_np = sess.run(y_est, feed_dict={X: Xtest, y: ytest})\n",
    "\n",
    "    #Calculate the prediction accuracy\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for estimate, target in zip(y_est_np, ytest.as_matrix()):\n",
    "      y_pred.append(estimate.argmax(axis=0))\n",
    "      y_true.append(target.argmax(axis=0))\n",
    "    accuracy = 100 * sum(correct) / len(correct)\n",
    "    print('accuracy', accuracy_score(y_true, y_pred), 'precision', precision_score(y_true, y_pred, average='weighted'), 'recall', recall_score(y_true, y_pred, average='weighted'), 'f1_score', f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
